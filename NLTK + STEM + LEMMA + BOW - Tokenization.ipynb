{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480657a5-abd0-4b59-a5a2-d624eaff82ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/om/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/om/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/om/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/om/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Tokenization Output:\n",
      " [\"'this is IT 7 sem H div',\\n       'students of IT sem 7 are good',\\n       'sem system is good foe education',\\n       'Education is good for human',\\n        'we want to carry semantic meaning',\\n       'understand the meaning of words',\\n       'It is a department'\"]\n",
      "\n",
      "Word Tokenization Output:\n",
      " [\"'this\", 'is', 'IT', '7', 'sem', 'H', 'div', \"'\", ',', \"'students\", 'of', 'IT', 'sem', '7', 'are', 'good', \"'\", ',', \"'sem\", 'system', 'is', 'good', 'foe', 'education', \"'\", ',', \"'Education\", 'is', 'good', 'for', 'human', \"'\", ',', \"'we\", 'want', 'to', 'carry', 'semantic', 'meaning', \"'\", ',', \"'understand\", 'the', 'meaning', 'of', 'words', \"'\", ',', \"'It\", 'is', 'a', 'department', \"'\"]\n",
      "\n",
      "Total Words = 53\n",
      "\n",
      "Words Printed Sentence by Sentence:\n",
      "\n",
      "'this\n",
      "is\n",
      "IT\n",
      "7\n",
      "sem\n",
      "H\n",
      "div',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'students\n",
      "of\n",
      "IT\n",
      "sem\n",
      "7\n",
      "are\n",
      "good',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'sem\n",
      "system\n",
      "is\n",
      "good\n",
      "foe\n",
      "education',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'Education\n",
      "is\n",
      "good\n",
      "for\n",
      "human',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'we\n",
      "want\n",
      "to\n",
      "carry\n",
      "semantic\n",
      "meaning',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'understand\n",
      "the\n",
      "meaning\n",
      "of\n",
      "words',\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'It\n",
      "is\n",
      "a\n",
      "department'\n",
      "\n",
      "Stemmed Sentences:\n",
      " [\"'thi 7 sem h div ' , 'student sem 7 good ' , 'sem system good foe educ ' , 'educ good human ' , 'we want carri semant mean ' , 'understand mean word ' , 'it depart '\"]\n",
      "\n",
      "Lemmatized Sentences:\n",
      " [\"'this 7 sem H div ' , 'students sem 7 good ' , 'sem system good foe education ' , 'Education good human ' , 'we want carry semantic meaning ' , 'understand meaning word ' , 'It department '\"]\n",
      "\n",
      "Stemming Output:\n",
      " ['three', 'vision', 'india', 'indian', 'peopl', 'peopl', '.']\n",
      "\n",
      "Lemmatization Output:\n",
      " ['three', 'vision', 'India', 'indian', 'people', 'people', '.']\n",
      "\n",
      "Stemmed Corpus:\n",
      " ['sem h div student sem good sem system good foe educ educ good human want carri semant mean understand mean word depart']\n",
      "\n",
      "Lemmatized Corpus:\n",
      " ['sem h div student sem good sem system good foe education education good human want carry semantic meaning understand meaning word department']\n",
      "\n",
      "Bag of Words Matrix:\n",
      "\n",
      "   carry  department  div  education  foe  good  human  meaning  sem  \\\n",
      "0      1           1    1          2    1     3      1        2    3   \n",
      "\n",
      "   semantic  student  system  understand  want  word  \n",
      "0         1        1       1           1     1     1  \n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# IMPORT REQUIRED LIBRARIES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import nltk                                      # Main NLP library\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize   # For word & sentence splitting\n",
    "from nltk.corpus import stopwords               # English stopwords\n",
    "from nltk.stem import PorterStemmer             # For stemming\n",
    "from nltk.stem import WordNetLemmatizer         # For lemmatization\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Bag-of-Words\n",
    "import re                                       # For cleaning text\n",
    "import pandas as pd                             # For DataFrame display\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DOWNLOAD REQUIRED NLTK RESOURCES\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "nltk.download('punkt')          # Tokenizer models\n",
    "nltk.download('stopwords')      # Stopword list\n",
    "nltk.download('wordnet')        # Dictionary for Lemmatizer\n",
    "nltk.download('punkt_tab') \n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAMPLE PARAGRAPH FOR NLP PROCESSING\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "paragraph = \"\"\"'this is IT 7 sem H div',\n",
    "       'students of IT sem 7 are good',\n",
    "       'sem system is good foe education',\n",
    "       'Education is good for human',\n",
    "        'we want to carry semantic meaning',\n",
    "       'understand the meaning of words',\n",
    "       'It is a department'\"\"\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1️⃣ SENTENCE TOKENIZATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "sentences = sent_tokenize(paragraph)   # Split paragraph into individual sentences\n",
    "print(\"\\nSentence Tokenization Output:\\n\", sentences)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2️⃣ WORD TOKENIZATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "words = word_tokenize(paragraph)       # Split text into individual words/punctuation\n",
    "print(\"\\nWord Tokenization Output:\\n\", words)\n",
    "print(\"\\nTotal Words =\", len(words))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3️⃣ PRINTING WORDS SPLIT BY SENTENCES (Manual split)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nWords Printed Sentence by Sentence:\\n\")\n",
    "for sentence in sentences:           \n",
    "    for word in sentence.split(\" \"):   # Split sentence by space\n",
    "        print(word)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4️⃣ STEMMING EXAMPLE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "stemmer = PorterStemmer()             # Initialize stemmer\n",
    "\n",
    "filtered_sentences = []               # To store processed sentences\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words_in_sentence = word_tokenize(sentences[i])   # Tokenize each sentence\n",
    "    # Remove stopwords + apply stemming\n",
    "    processed_words = [\n",
    "        stemmer.stem(word)            # Apply stemming\n",
    "        for word in words_in_sentence\n",
    "        if word.lower() not in stopwords.words(\"english\")  # Remove stopwords\n",
    "    ]\n",
    "    filtered_sentences.append(' '.join(processed_words))  # Join back into text\n",
    "\n",
    "print(\"\\nStemmed Sentences:\\n\", filtered_sentences)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5️⃣ LEMMATIZATION EXAMPLE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()      # Initialize lemmatizer\n",
    "\n",
    "lemmatized_sentences = []             # To store processed sentences\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words_in_sentence = word_tokenize(sentences[i])\n",
    "    # Remove stopwords + apply lemmatization\n",
    "    processed_words = [\n",
    "        lemmatizer.lemmatize(word)    # Apply lemmatization\n",
    "        for word in words_in_sentence\n",
    "        if word.lower() not in stopwords.words(\"english\")\n",
    "    ]\n",
    "    lemmatized_sentences.append(' '.join(processed_words))\n",
    "\n",
    "print(\"\\nLemmatized Sentences:\\n\", lemmatized_sentences)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6️⃣ STEMMING VS LEMMATIZATION EXAMPLE ON CUSTOM TEXT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "text = \"I have three visions for India and indian people and peoples.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stemmed_output = [\n",
    "    stemmer.stem(word)\n",
    "    for word in word_tokens\n",
    "    if word.lower() not in stopwords.words('english')\n",
    "]\n",
    "\n",
    "# Lemmatization\n",
    "lemmatized_output = [\n",
    "    lemmatizer.lemmatize(word)\n",
    "    for word in word_tokens\n",
    "    if word.lower() not in stopwords.words('english')\n",
    "]\n",
    "\n",
    "print(\"\\nStemming Output:\\n\", stemmed_output)\n",
    "print(\"\\nLemmatization Output:\\n\", lemmatized_output)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7️⃣ CLEAN THE PARAGRAPH FOR BAG-OF-WORDS (STEMMING VERSION)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "sentences = sent_tokenize(paragraph)\n",
    "corpus_stem = []                      # Store cleaned & stemmed sentences\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])  # Remove non-letters\n",
    "    review = review.lower()                           # Convert to lowercase\n",
    "    review = review.split()                           # Split into words\n",
    "    # Remove stopwords + apply stemming\n",
    "    review = [stemmer.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)                         # Join words back\n",
    "    corpus_stem.append(review)\n",
    "\n",
    "print(\"\\nStemmed Corpus:\\n\", corpus_stem)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8️⃣ CLEAN THE PARAGRAPH FOR BAG-OF-WORDS (LEMMATIZATION VERSION)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "corpus_lemma = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    # Remove stopwords + apply lemmatization\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus_lemma.append(review)\n",
    "\n",
    "print(\"\\nLemmatized Corpus:\\n\", corpus_lemma)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 9️⃣ BAG OF WORDS USING CountVectorizer\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "cv = CountVectorizer()                          # Create BOW model\n",
    "X = cv.fit_transform(corpus_lemma).toarray()    # Convert text → vectors\n",
    "\n",
    "df_bow = pd.DataFrame(X, columns=cv.get_feature_names_out())   # Create DataFrame\n",
    "print(\"\\nBag of Words Matrix:\\n\")\n",
    "print(df_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7171b2b-5e4f-4eb7-b002-7bc23e398d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
